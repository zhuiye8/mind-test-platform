import threading
import time
import cv2
import numpy as np
from datetime import datetime
import subprocess
import os
from shutil import which
from typing import Optional, Callable, Dict, Any

# è½»é‡æ—¥å¿—æ§åˆ¶ï¼ˆé»˜è®¤ INFOï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ AI_LOG_LEVEL=DEBUG/INFO/WARN/ERROR è°ƒæ•´ï¼‰
_LEVELS = {"DEBUG": 10, "INFO": 20, "WARN": 30, "ERROR": 40}
_LOG_LEVEL = _LEVELS.get(os.environ.get('AI_LOG_LEVEL', 'INFO').upper(), 10)

def _log_debug(msg: str):
    if _LOG_LEVEL <= 10:
        print(msg)

def _log_info(msg: str):
    if _LOG_LEVEL <= 20:
        print(msg)

def _log_warn(msg: str):
    if _LOG_LEVEL <= 30:
        print(msg)

def _log_error(msg: str):
    if _LOG_LEVEL <= 40:
        print(msg)

# å¯¼å…¥DataManagerç”¨äºå®æ—¶ä¿å­˜æ•°æ®
try:
    from utils.data_manager import DataManager
    data_manager = DataManager()
    _log_info("[RTSP] DataManagerå·²å¯¼å…¥ï¼Œå°†å®æ—¶ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶")
except Exception as e:
    data_manager = None
    _log_warn(f"[RTSP] DataManagerå¯¼å…¥å¤±è´¥: {e}, å°†ä½¿ç”¨å†…å­˜ç¼“å†²")

try:
    from contract_api.callbacks import callback_service as _cb
except Exception:
    _cb = None

def _maybe_send_checkpoint(session_id: Optional[str], model: str, payload: Dict[str, Any], min_interval: float = 1.0):
    """ä½¿ç”¨DataManagerå®æ—¶ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        if not session_id or not data_manager:
            return
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨å¯¹åº”çš„DataManageræ–¹æ³•
        if 'video' in model.lower() or 'face' in model.lower():
            data_manager.add_video_emotion(session_id, {
                'dominant_emotion': payload.get('dominant_emotion', 'neutral'),
                'emotions': payload.get('emotions', {}),
                'confidence': payload.get('confidence', 0.0),
                'face_detected': payload.get('face_detected', True)
            })
            _log_debug(f"[RTSP] ä¿å­˜è§†é¢‘æƒ…ç»ª: {session_id}, ä¸»å¯¼æƒ…ç»ª: {payload.get('dominant_emotion')}")
            
        elif 'audio' in model.lower() or 'voice' in model.lower():
            data_manager.add_audio_emotion(session_id, {
                'dominant_emotion': payload.get('dominant_emotion', 'neutral'),
                'emotions': payload.get('emotions', {}),
                'confidence': payload.get('confidence', 0.0)
            })
            _log_debug(f"[RTSP] ä¿å­˜éŸ³é¢‘æƒ…ç»ª: {session_id}, ä¸»å¯¼æƒ…ç»ª: {payload.get('dominant_emotion')}")
            
        elif 'heart' in model.lower() or 'ppg' in model.lower():
            data_manager.add_heart_rate_data(session_id, {
                'heart_rate': payload.get('heart_rate', 0),
                'confidence': payload.get('confidence', 0.0),
                'signal_length': payload.get('signal_length', 0)
            })
            _log_debug(f"[RTSP] ä¿å­˜å¿ƒç‡æ•°æ®: {session_id}, å¿ƒç‡: {payload.get('heart_rate')}")
            
    except Exception as e:
        _log_warn(f"[RTSP] DataManagerä¿å­˜å¤±è´¥: {e}")

def ensure_session_created(session_id: str):
    """ç¡®ä¿DataManagerä¸­å­˜åœ¨è¯¥ä¼šè¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if data_manager and session_id:
        try:
            # å°è¯•åŠ è½½ä¼šè¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
            existing = data_manager.load_session(session_id)
            if not existing:
                _log_info(f"[RTSP] åˆ›å»ºæ–°çš„DataManagerä¼šè¯: {session_id}")
                data_manager.create_session(session_id)
            else:
                _log_debug(f"[RTSP] ä¼šè¯å·²å­˜åœ¨: {session_id}")
        except Exception as e:
            _log_warn(f"[RTSP] æ£€æŸ¥/åˆ›å»ºä¼šè¯å¤±è´¥: {e}")

_socketio = None
_app = None
_session_mapper: Optional[Callable[[str], Optional[Dict[str, Any]]]] = None

# è½»é‡çº§æœ€æ–°çŠ¶æ€ç¼“å­˜ï¼ˆç”¨äºHTTPè½®è¯¢/SSEä¸‹è¡Œï¼‰
_latest_state: Dict[str, Dict[str, Any]] = {}
_state_lock = threading.Lock()
_state_version = 0


def set_socketio(socketio, app=None):
    global _socketio, _app
    _socketio = socketio
    _app = app
    _log_debug(f"[RTSP] Socket.IO å·²è®¾ç½®: {socketio is not None}, App å·²è®¾ç½®: {app is not None}")

def set_session_mapper(mapper: Callable[[str], Optional[Dict[str, Any]]]):
    """æ³¨å†Œä» stream_name æ˜ å°„åˆ°å­¦ç”Ÿä¼šè¯çš„å›è°ƒã€‚
    mapper(stream_name) -> { 'session_id': str, 'student_id': str } or None
    """
    global _session_mapper
    _session_mapper = mapper


def _bump_version() -> int:
    global _state_version
    _state_version += 1
    return _state_version


def _update_state(stream_name: str, key: str, payload: Dict[str, Any]):
    """æ›´æ–°æŸä¸ªæµçš„æœ€æ–°çŠ¶æ€ã€‚
    key: 'video' | 'heart'
    payload: å¯åºåˆ—åŒ–çš„ç»“æœæ•°æ®
    """
    ts = datetime.now().isoformat()
    with _state_lock:
        st = _latest_state.get(stream_name) or {
            'stream_name': stream_name,
            'version': 0,
            'updated_at': ts,
            'video': None,
            'heart': None,
        }
        st[key] = payload
        st['updated_at'] = ts
        st['version'] = _bump_version()
        _latest_state[stream_name] = st


def get_latest_state(stream_name: Optional[str]) -> Optional[Dict[str, Any]]:
    """è·å–æŸæ¡æµçš„æœ€æ–°çŠ¶æ€ï¼ˆæµ…æ‹·è´ï¼‰ã€‚"""
    if not stream_name:
        return None
    with _state_lock:
        st = _latest_state.get(stream_name)
        if not st:
            return None
        # è¿”å›æµ…æ‹·è´ï¼Œé¿å…å¹¶å‘è¯»å†™é—®é¢˜
        out = dict(st)
        if out.get('video') is not None:
            out['video'] = dict(out['video'])
        if out.get('heart') is not None:
            out['heart'] = dict(out['heart'])
        return out


def _safe_emit(event, data, **kwargs):
    """åœ¨åå°çº¿ç¨‹ä¸­å®‰å…¨åœ°å‘é€Socket.IOäº‹ä»¶"""
    if _socketio is None:
        _log_warn(f"[RTSP] âŒ Socket.IOæœªåˆå§‹åŒ–ï¼Œæ— æ³•å‘é€äº‹ä»¶: {event}")
        return False
    
    try:
        if _app is not None:
            # ä½¿ç”¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡
            with _app.app_context():
                _socketio.emit(event, data, **kwargs)
                return True
        else:
            # å¦‚æœæ²¡æœ‰appå®ä¾‹ï¼Œç›´æ¥å°è¯•å‘é€
            _socketio.emit(event, data, **kwargs)
            return True
    except Exception as e:
        _log_warn(f"[RTSP] âŒ å‘é€äº‹ä»¶å¤±è´¥ {event}: {e}")
        return False


class _ConsumerThread(threading.Thread):
    def __init__(self, stream_name: str, rtsp_url: str, model_manager):
        super().__init__(daemon=True)
        self.stream_name = stream_name
        self.rtsp_url = rtsp_url
        self.model_manager = model_manager
        self._stop = threading.Event()
        self.connected = False
        self.last_frame_ts = 0.0
        self._ff_proc = None
        self._ff_w = 640
        self._ff_h = 360
        # éŸ³é¢‘ç›¸å…³
        self._ff_proc_audio = None
        self._audio_thread = None
        self._audio_buf = bytearray()
        self._audio_sr = 16000
        self._audio_channels = 1
        self._audio_bytes_per_sample = 2  # s16le
        self._audio_chunk_sec = 2.0  # æ¯æ®µ2ç§’
        self._audio_bytes_read = 0
        self._audio_chunks = 0
        self._audio_last_ts = 0.0

    def _open_capture(self):
        # å°è¯•åŸå§‹URL ä¸ TCPä¼˜å…ˆURL
        urls = [self.rtsp_url]
        if 'rtsp_transport=' not in self.rtsp_url:
            sep = '&' if '?' in self.rtsp_url else '?'
            urls.append(f"{self.rtsp_url}{sep}rtsp_transport=tcp")
        for i in range(2):  # æ¯è½®å°è¯•æ‰€æœ‰å€™é€‰URL
            for u in urls:
                if self._stop.is_set():
                    return None
                cap = cv2.VideoCapture(u)
                if cap.isOpened():
                    if u != self.rtsp_url:
                        print(f"[RTSP] ä½¿ç”¨TCPè¿æ¥æˆåŠŸ: {u}")
                    return cap
                try:
                    cap.release()
                except Exception:
                    pass
        return None

    def _start_ffmpeg(self):
        if self._ff_proc is not None:
            return True
        try:
            url = self.rtsp_url
            if 'rtsp_transport=' not in url:
                sep = '&' if '?' in url else '?'
                url = f"{url}{sep}rtsp_transport=tcp"
            ffbin = os.environ.get('FFMPEG_BIN') or which('ffmpeg') or '/usr/bin/ffmpeg'
            if not os.path.exists(ffbin):
                _log_warn(f"[RTSP/FFmpeg] æœªæ‰¾åˆ° ffmpeg å¯æ‰§è¡Œæ–‡ä»¶ï¼ˆFFMPEG_BIN={os.environ.get('FFMPEG_BIN','')}). å°†ä¸å¯ç”¨FFmpegå…œåº•è§£ç ")
                self._ff_proc = None
                return False
            cmd = [
                ffbin, '-nostdin', '-hide_banner', '-loglevel', 'warning',
                '-rtsp_transport', 'tcp', '-i', url,
                '-an', '-vf', f'scale={self._ff_w}:{self._ff_h}',
                '-pix_fmt', 'bgr24', '-f', 'rawvideo', 'pipe:1'
            ]
            self._ff_proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=self._ff_w*self._ff_h*3)
            _log_info(f"[RTSP/FFmpeg] å·²å¯åŠ¨FFmpegè§£ç ç®¡é“")
            return True
        except Exception as e:
            _log_warn(f"[RTSP/FFmpeg] å¯åŠ¨å¤±è´¥: {e}")
            self._ff_proc = None
            return False

    def _start_ffmpeg_audio(self):
        if self._ff_proc_audio is not None:
            return True
        try:
            url = self.rtsp_url
            if 'rtsp_transport=' not in url:
                sep = '&' if '?' in url else '?'
                url = f"{url}{sep}rtsp_transport=tcp"
            ffbin = os.environ.get('FFMPEG_BIN') or which('ffmpeg') or '/usr/bin/ffmpeg'
            if not os.path.exists(ffbin):
                _log_warn(f"[RTSP/FFmpeg-Audio] æœªæ‰¾åˆ° ffmpeg å¯æ‰§è¡Œæ–‡ä»¶ï¼ŒéŸ³é¢‘åˆ†æç¦ç”¨")
                self._ff_proc_audio = None
                return False
            # è¾“å‡º16kHzå•å£°é“s16leåŸå§‹PCMåˆ°stdout
            cmd = [
                ffbin, '-nostdin', '-hide_banner', '-loglevel', 'warning',
                '-rtsp_transport', 'tcp', '-i', url,
                '-vn', '-ac', str(self._audio_channels), '-ar', str(self._audio_sr),
                '-f', 's16le', 'pipe:1'
            ]
            self._ff_proc_audio = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            _log_info(f"[RTSP/FFmpeg-Audio] å·²å¯åŠ¨FFmpegéŸ³é¢‘è§£ç ç®¡é“")
            return True
        except Exception as e:
            _log_warn(f"[RTSP/FFmpeg-Audio] å¯åŠ¨å¤±è´¥: {e}")
            self._ff_proc_audio = None
            return False

    def _audio_worker(self):
        """è¯»å–éŸ³é¢‘PCMå¹¶åˆ†æ®µåˆ†æï¼ˆEmotion2Vecï¼‰"""
        try:
            if not self._start_ffmpeg_audio():
                return
            chunk_bytes = int(self._audio_sr * self._audio_chunk_sec) * self._audio_bytes_per_sample
            last_emit = 0.0
            while not self._stop.is_set():
                if self._ff_proc_audio is None or self._ff_proc_audio.stdout is None:
                    time.sleep(0.05)
                    continue
                data = self._ff_proc_audio.stdout.read(4096)
                if not data:
                    time.sleep(0.01)
                    continue
                self._audio_buf.extend(data)
                self._audio_bytes_read += len(data)
                while len(self._audio_buf) >= chunk_bytes:
                    chunk = self._audio_buf[:chunk_bytes]
                    del self._audio_buf[:chunk_bytes]
                    try:
                        # s16le -> float32/64 numpy æ³¢å½¢ï¼Œå•å£°é“
                        import numpy as _np
                        audio_np = _np.frombuffer(chunk, dtype=_np.int16).astype(_np.float32) / 32768.0
                        # åˆ†æ
                        emo2v = self.model_manager.get_emotion2vec_analyzer()
                        if not emo2v.is_initialized:
                            emo2v.initialize()
                        res = emo2v.analyze_array(audio_np, sample_rate=self._audio_sr)
                        res['timestamp'] = datetime.now().isoformat()
                        # ç²¾ç®€è´Ÿè½½ï¼Œé¿å…è¿‡å¤§å¯¼è‡´ä¼ è¾“å¼‚å¸¸
                        res_emit = {
                            'emotions': res.get('emotions') or {},
                            'dominant_emotion': res.get('dominant_emotion'),
                            'confidence': res.get('confidence'),
                            'model': res.get('model'),
                            'analysis_quality': res.get('analysis_quality') or 'high',
                            'timestamp': res.get('timestamp')
                        }
                        self._audio_chunks += 1
                        self._audio_last_ts = time.time()
                        # çŠ¶æ€ç¼“å­˜
                        try:
                            _update_state(self.stream_name, 'audio', res_emit)
                        except Exception:
                            pass
                        # ç»„è£…payload
                        payload = {
                            'session_id': self.stream_name,
                            'stream_name': self.stream_name,
                            'result': res_emit
                        }
                        now = time.time()
                        if _safe_emit('audio_emotion_result', payload):
                            if (now - last_emit) > 1.0:
                                _log_debug(f"[RTSP/AUDIO] é»˜è®¤å‘½åç©ºé—´ audio_emotion_result: stream={self.stream_name}, dom={res.get('dominant_emotion')}")
                        # å¤‡ç”¨äº‹ä»¶åï¼ˆä¸è§†é¢‘/å¿ƒç‡ä¸€è‡´çš„æ¨¡å¼ï¼‰
                        _safe_emit('rtsp_audio_analysis', payload)
                        # å¹¿æ’­åˆ°/monitorï¼ˆå¹¶æ¨é€åˆ°æˆ¿é—´ stream:<name> çš„å­¦ç”ŸéŸ³é¢‘äº‹ä»¶ï¼‰
                        _safe_emit('audio_emotion_result', payload, namespace='/monitor')
                        _safe_emit('rtsp_audio_analysis', payload, namespace='/monitor')
                        room = f"stream:{self.stream_name}"
                        _safe_emit('student.audio', payload, room=room, namespace='/monitor')
                        # å­¦ç”Ÿå®šå‘/æ•™å¸ˆäº‹ä»¶
                        if _session_mapper is not None:
                            info = _session_mapper(self.stream_name) or {}
                            sid = info.get('session_id')
                            student_id = info.get('student_id')
                            sid_default = info.get('sid_default')
                            # å‘åç«¯å‘é€â€œéŸ³é¢‘æƒ…ç»ªâ€æ£€æŸ¥ç‚¹ï¼ˆèŠ‚æµï¼‰
                        # ä¿åº•å†™å…¥ï¼šè‹¥æœªæ˜ å°„åˆ°å­¦ç”Ÿä¼šè¯IDï¼Œåˆ™ä½¿ç”¨ stream_name ä½œä¸ºä¼šè¯é”®ï¼Œç¡®ä¿ç¼“å†²æœ‰æ•°æ®
                        if sid:
                            ensure_session_created(sid)
                            _maybe_send_checkpoint(sid, 'audio_emotion', {
                                'dominant_emotion': res_emit.get('dominant_emotion'),
                                'confidence': res_emit.get('confidence'),
                                'emotions': res_emit.get('emotions'),
                            })
                        else:
                            ensure_session_created(self.stream_name)
                            _maybe_send_checkpoint(self.stream_name, 'audio_emotion', {
                                'dominant_emotion': res_emit.get('dominant_emotion'),
                                'confidence': res_emit.get('confidence'),
                                'emotions': res_emit.get('emotions'),
                            })
                            sid_monitor = info.get('sid_monitor')
                            if sid:
                                _safe_emit('student_audio_emotion_result', {
                                    'session_id': sid,
                                    'student_id': student_id,
                                    'result': res_emit
                                })
                            if sid_default:
                                payload_target = dict(payload)
                                if sid:
                                    payload_target['session_id'] = sid
                                if _safe_emit('audio_emotion_result', payload_target, room=sid_default):
                                    _log_debug(f"[RTSP/AUDIO] å®šå‘æ¨é€åˆ°é»˜è®¤å‘½åç©ºé—´ audio_emotion_result: sid_default={sid_default}")
                            if sid_monitor:
                                # å®šå‘æ¨é€åˆ°/monitor ä¸Šè¯¥æµè§ˆå™¨è¿æ¥
                                if _safe_emit('audio_emotion_result', payload, room=sid_monitor, namespace='/monitor'):
                                    _log_debug(f"[RTSP/AUDIO] å®šå‘æ¨é€åˆ°/monitor audio_emotion_result: sid_monitor={sid_monitor}")
                        last_emit = now
                    except Exception as e:
                        _log_warn(f"[RTSP/AUDIO] åˆ†æå¤±è´¥: {e}")
        finally:
            if self._ff_proc_audio is not None:
                try:
                    self._ff_proc_audio.kill()
                except Exception:
                    pass
                self._ff_proc_audio = None

    def _read_ffmpeg_frame(self):
        if self._ff_proc is None or self._ff_proc.stdout is None:
            return False, None
        try:
            size = self._ff_w * self._ff_h * 3
            data = self._ff_proc.stdout.read(size)
            if not data or len(data) < size:
                return False, None
            frame = np.frombuffer(data, np.uint8).reshape((self._ff_h, self._ff_w, 3))
            return True, frame
        except Exception:
            return False, None

    def run(self):
        backoff = 0.5
        cap = None
        last_emit = 0.0
        self.last_frame_ts = time.time()
        empty_reads = 0
        last_diag = 0.0

        # æŒç»­é‡è¯•æ‰“å¼€ï¼Œç›´åˆ°æˆåŠŸæˆ–è¢«åœæ­¢
        while not self._stop.is_set() and cap is None:
            cap = self._open_capture()
            if cap is None:
                _log_warn(f"[RTSP] å°šæœªå¯ç”¨ï¼Œç­‰å¾…åé‡è¯•: {self.rtsp_url}")
                time.sleep(backoff)
                backoff = min(backoff * 2, 5.0)

        if self._stop.is_set():
            return

        _log_info(f"[RTSP] å¼€å§‹æ¶ˆè´¹: {self.rtsp_url}")
        self.connected = True
        # å¯åŠ¨éŸ³é¢‘åˆ†æçº¿ç¨‹ï¼ˆä¸è§†é¢‘å¹¶è¡Œï¼‰
        try:
            if self._audio_thread is None or not self._audio_thread.is_alive():
                self._audio_thread = threading.Thread(target=self._audio_worker, daemon=True)
                self._audio_thread.start()
        except Exception as _e:
            _log_warn(f"[RTSP] å¯åŠ¨éŸ³é¢‘çº¿ç¨‹å¤±è´¥: {_e}")
        try:
            while not self._stop.is_set():
                ok, frame = (cap.read() if cap is not None else (False, None))
                if not ok and self._ff_proc is not None:
                    ok, frame = self._read_ffmpeg_frame()
                if not ok or frame is None:
                    empty_reads += 1
                    # è¿ç»­è¯»ç©ºå¸§ï¼Œé‡è¿
                    if empty_reads >= 30:  # ~1.5s
                        try:
                            cap.release()
                        except Exception:
                            pass
                        cap = None
                        self.connected = False
                        backoff = 0.5
                        # å¯åŠ¨FFmpegä½œä¸ºå¤‡ç”¨è§£ç 
                        if self._ff_proc is None:
                            self._start_ffmpeg()
                        while not self._stop.is_set() and cap is None:
                            cap = self._open_capture()
                            if cap is None:
                                if (time.time() - last_diag) > 2.0:
                                    _log_warn(f"[RTSP] è¯»å–å¤±è´¥ï¼Œæ­£åœ¨é‡è¿: {self.rtsp_url}")
                                    last_diag = time.time()
                                time.sleep(backoff)
                                backoff = min(backoff * 2, 5.0)
                        if cap is not None:
                            self.connected = True
                        empty_reads = 0
                        continue
                    time.sleep(0.05)
                    continue
                empty_reads = 0
                self.last_frame_ts = time.time()

                # å‘¨æœŸæ€§è¯Šæ–­ï¼šç¡®è®¤å¸§åœ¨è¯»å–
                if (self.last_frame_ts - last_diag) > 2.0:
                    print(f"[RTSP] å·²è¯»å–è§†é¢‘å¸§: stream={self.stream_name}, ts={self.last_frame_ts:.2f}")
                    last_diag = self.last_frame_ts

                # é™é‡‡æ ·å¤„ç†ï¼Œé™ä½å¼€é”€
                h, w = frame.shape[:2]
                scale = min(1.0, 640.0 / max(1, w))
                if scale < 1.0:
                    frame = cv2.resize(frame, (int(w*scale), int(h*scale)))

                # åˆ†æï¼ˆå®¹é”™ï¼‰
                result = {
                    'timestamp': datetime.now().isoformat(),
                    'dominant_emotion': 'unknown',
                    'confidence': 0.0
                }
                try:
                    deepface_analyzer = self.model_manager.get_deepface_analyzer()
                    if deepface_analyzer is not None:
                        if hasattr(deepface_analyzer, 'is_initialized') and not deepface_analyzer.is_initialized:
                            if hasattr(deepface_analyzer, 'initialize'):
                                deepface_analyzer.initialize()
                        # BGR -> RGB
                        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        r = deepface_analyzer.analyze(rgb)
                        result.update(r)
                except Exception:
                    # åˆ†æå¤±è´¥ä¸é˜»æ–­
                    pass

                # é™é¢‘å‘é€ï¼ˆæ¯ç§’æœ€å¤š 2 æ¬¡ï¼Œé™ä½é¢‘ç‡é¿å…å‰ç«¯æ¸²æŸ“é—®é¢˜ï¼‰
                now = time.time()
                if _socketio is not None and (now - last_emit) > 0.5:
                    payload = {
                        'session_id': self.stream_name,
                        'stream_name': self.stream_name,
                        'result': result,
                        'video_timestamp': now
                    }
                    # æ›´æ–°HTTPè½®è¯¢å¯è¯»çš„æœ€æ–°çŠ¶æ€ï¼ˆè§†é¢‘ï¼‰
                    try:
                        _update_state(self.stream_name, 'video', result)
                    except Exception:
                        pass
                    # å¹¿æ’­ç»™é»˜è®¤å‘½åç©ºé—´çš„æ‰€æœ‰å®¢æˆ·ç«¯ - ä½¿ç”¨åº”ç”¨ä¸Šä¸‹æ–‡ï¼ˆé™é¢‘æ‰“å°ï¼‰
                    if _safe_emit('video_emotion_result', payload):
                        if (now - last_diag) > 2.0:
                            print(f"[RTSP] âœ… å¹¿æ’­é»˜è®¤NS video_emotion_result: stream={self.stream_name}, dom={result.get('dominant_emotion')}")
                    
                    # é¢å¤–ç¡®è®¤ï¼šä¹Ÿå°è¯•å¹¿æ’­åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
                    if _safe_emit('rtsp_video_analysis', payload):
                        if (now - last_diag) > 2.0:
                            print(f"[RTSP] âœ… é¢å¤–å¹¿æ’­ rtsp_video_analysis")

                    # åŒæ—¶å¹¿æ’­åˆ°ç›‘æ§å‘½åç©ºé—´ï¼ˆä¿ç•™ï¼‰
                    if _safe_emit('video_emotion_result', payload, namespace='/monitor'):
                        if (now - last_diag) > 2.0:
                            print(f"[RTSP] âœ… å¹¿æ’­/monitor video_emotion_result: stream={self.stream_name}")
                    
                    # é¢å¤–ç¡®è®¤ï¼šä¹Ÿå°è¯•åœ¨monitorå‘½åç©ºé—´å¹¿æ’­å¤‡ç”¨äº‹ä»¶
                    if _safe_emit('rtsp_video_analysis', payload, namespace='/monitor'):
                        if (now - last_diag) > 2.0:
                            print(f"[RTSP] âœ… /monitor é¢å¤–å¹¿æ’­ rtsp_video_analysis")

                    # å‘é€åˆ°ç‰¹å®šæˆ¿é—´ï¼ˆä¿ç•™æˆ¿é—´æœºåˆ¶ï¼‰
                    room = f"stream:{self.stream_name}"
                    if _safe_emit('student.emotion', payload, room=room, namespace='/monitor'):
                        if (now - last_diag) > 1.5:
                            print(f"[RTSP] å·²å‘é€ student.emotion è‡³æˆ¿é—´: {room}")

                    # åŒæ­¥è½¬å‘ç»™æ•™å¸ˆç«¯ï¼ˆstudent_* äº‹ä»¶ï¼‰ï¼Œéœ€è¦å°† stream_name æ˜ å°„ä¸ºå­¦ç”Ÿä¼šè¯ID
                    if _session_mapper is not None:
                        try:
                            info = _session_mapper(self.stream_name)
                            if info and isinstance(info, dict):
                                sid = info.get('session_id')
                                student_id = info.get('student_id')
                                sid_default = info.get('sid_default')
                                if sid:
                                    if _safe_emit('student_video_emotion_result', {
                                        'session_id': sid,
                                        'student_id': student_id,
                                        'result': result
                                    }):
                                        if (now - last_diag) > 1.5:
                                            print(f"[RTSP] å·²è½¬å‘ student_video_emotion_result: sid={sid}, student_id={student_id}")
                                # å…³é”®ï¼šå¤ç”¨æœ¬æœºæ£€æµ‹é€šè·¯ï¼Œå®šå‘æ¨é€åˆ°é»˜è®¤å‘½åç©ºé—´çš„ç‰¹å®šæµè§ˆå™¨è¿æ¥
                                if sid_default:
                                    payload_target = dict(payload)
                                    if sid:
                                        payload_target['session_id'] = sid  # ä¸å½“å‰ç›‘æ§å­¦ç”Ÿä¼šè¯IDå¯¹é½
                                    if _safe_emit('video_emotion_result', payload_target, room=sid_default):
                                        print(f"[RTSP] ğŸ¯ å®šå‘æ¨é€åˆ°é»˜è®¤å‘½åç©ºé—´ video_emotion_result: sid_default={sid_default}")
                                # å‘é€è§†é¢‘æƒ…ç»ªæ£€æŸ¥ç‚¹ï¼ˆèŠ‚æµï¼Œ1sä¸€æ¬¡ï¼‰
                                # å‘é€è§†é¢‘æƒ…ç»ªæ£€æŸ¥ç‚¹ï¼šä¼˜å…ˆæŒ‰ä¼šè¯IDï¼›æ— ä¼šè¯IDåˆ™ä»¥ stream_name ç¼“å­˜
                                if sid:
                                    ensure_session_created(sid)
                                    _maybe_send_checkpoint(sid, 'video_emotion', {
                                        'dominant_emotion': result.get('dominant_emotion'),
                                        'confidence': result.get('confidence'),
                                        'emotions': result.get('emotions'),
                                        'face_detected': result.get('face_detected'),
                                    })
                                else:
                                    ensure_session_created(self.stream_name)
                                    _maybe_send_checkpoint(self.stream_name, 'video_emotion', {
                                        'dominant_emotion': result.get('dominant_emotion'),
                                        'confidence': result.get('confidence'),
                                        'emotions': result.get('emotions'),
                                        'face_detected': result.get('face_detected'),
                                    })
                        except Exception:
                            pass

                    # è§¦å‘ PPG å¿ƒç‡æ£€æµ‹å¹¶å‘é€ç»“æœï¼ˆè½»é‡é¢‘ç‡ï¼Œä¸å¼ºåˆ¶æ¯å¸§ï¼‰
                    try:
                        from models.enhanced_ppg_detector import enhanced_ppg_detector
                        hr = enhanced_ppg_detector.process_frame(rgb if 'rgb' in locals() else frame, bool(result.get('face_detected')))
                        hr['timestamp'] = datetime.now().isoformat()
                        base_hr_payload = {
                            'session_id': self.stream_name,
                            'stream_name': self.stream_name,
                            'result': hr
                        }
                        # æ›´æ–°HTTPè½®è¯¢å¯è¯»çš„æœ€æ–°çŠ¶æ€ï¼ˆå¿ƒç‡ï¼‰
                        try:
                            _update_state(self.stream_name, 'heart', hr)
                        except Exception:
                            pass
                        # å¹¿æ’­ç»™é»˜è®¤å‘½åç©ºé—´çš„æ‰€æœ‰å®¢æˆ·ç«¯ - ä½¿ç”¨åº”ç”¨ä¸Šä¸‹æ–‡
                        if _safe_emit('heart_rate_result', base_hr_payload):
                            if (now - last_diag) > 2.0:
                                print(f"[RTSP] âœ… å¹¿æ’­é»˜è®¤NS heart_rate_result: stream={self.stream_name}, state={hr.get('detection_state')}")
                        
                        # é¢å¤–ç¡®è®¤ï¼šä¹Ÿå°è¯•å¹¿æ’­å¿ƒç‡åˆ†æäº‹ä»¶
                        if _safe_emit('rtsp_heart_rate_analysis', base_hr_payload):
                            if (now - last_diag) > 2.0:
                                print(f"[RTSP] âœ… é¢å¤–å¹¿æ’­ rtsp_heart_rate_analysis")
                        
                        # åŒæ—¶å¹¿æ’­åˆ°ç›‘æ§å‘½åç©ºé—´ï¼ˆå…³é”®ä¿®å¤ï¼‰
                        if _safe_emit('heart_rate_result', base_hr_payload, namespace='/monitor'):
                            if (now - last_diag) > 2.0:
                                print(f"[RTSP] âœ… å¹¿æ’­/monitor heart_rate_result: stream={self.stream_name}")
                        
                        # é¢å¤–ç¡®è®¤ï¼šä¹Ÿå°è¯•åœ¨monitorå‘½åç©ºé—´å¹¿æ’­å¤‡ç”¨äº‹ä»¶
                        if _safe_emit('rtsp_heart_rate_analysis', base_hr_payload, namespace='/monitor'):
                            if (now - last_diag) > 2.0:
                                _log_debug(f"[RTSP] /monitor é¢å¤–å¹¿æ’­ rtsp_heart_rate_analysis")
                            
                        # å‘é€åˆ°ç‰¹å®šæˆ¿é—´ï¼ˆä¿ç•™æˆ¿é—´æœºåˆ¶ï¼‰
                        room = f"stream:{self.stream_name}"
                        if _safe_emit('student.heart_rate', base_hr_payload, room=room, namespace='/monitor'):
                            if (now - last_diag) > 1.5:
                                _log_debug(f"[RTSP] å·²å‘é€ student.heart_rate è‡³æˆ¿é—´: {room}")
                        if _session_mapper is not None:
                            info = _session_mapper(self.stream_name)
                            if info and isinstance(info, dict):
                                sid = info.get('session_id')
                                student_id = info.get('student_id')
                                sid_default = info.get('sid_default')
                                if sid:
                                    if _safe_emit('student_heart_rate_result', {
                                        'session_id': sid,
                                        'student_id': student_id,
                                        'result': hr
                                    }):
                                        if (now - last_diag) > 1.5:
                                            _log_debug(f"[RTSP] å·²è½¬å‘ student_heart_rate_result: sid={sid}, hr={hr.get('heart_rate')}")
                                # å…³é”®ï¼šå¤ç”¨æœ¬æœºæ£€æµ‹é€šè·¯ï¼Œå®šå‘æ¨é€åˆ°é»˜è®¤å‘½åç©ºé—´çš„ç‰¹å®šæµè§ˆå™¨è¿æ¥
                                if sid_default:
                                    payload_target = dict(base_hr_payload)
                                    if sid:
                                        payload_target['session_id'] = sid
                                    if _safe_emit('heart_rate_result', payload_target, room=sid_default):
                                        _log_debug(f"[RTSP] å®šå‘æ¨é€åˆ°é»˜è®¤å‘½åç©ºé—´ heart_rate_result: sid_default={sid_default}")
                                # å‘é€å¿ƒç‡æ£€æŸ¥ç‚¹ï¼ˆèŠ‚æµï¼‰ï¼Œä»¥ä¾¿åç«¯å®æ—¶å…¥åº“
                                # å‘é€å¿ƒç‡æ£€æŸ¥ç‚¹ï¼šä¼˜å…ˆæŒ‰ä¼šè¯IDï¼›æ— ä¼šè¯IDåˆ™ä»¥ stream_name ç¼“å­˜
                                if sid:
                                    ensure_session_created(sid)
                                    _maybe_send_checkpoint(sid, 'ppg_detector', {
                                        'heart_rate': hr.get('heart_rate') or hr.get('hr_bpm'),
                                        'confidence': hr.get('confidence'),
                                        'detection_state': hr.get('detection_state') or hr.get('state'),
                                    })
                                else:
                                    ensure_session_created(self.stream_name)
                                    _maybe_send_checkpoint(self.stream_name, 'ppg_detector', {
                                        'heart_rate': hr.get('heart_rate') or hr.get('hr_bpm'),
                                        'confidence': hr.get('confidence'),
                                        'detection_state': hr.get('detection_state') or hr.get('state'),
                                    })
                    except Exception:
                        pass
                    last_emit = now
        finally:
            if cap is not None:
                try:
                    cap.release()
                except Exception:
                    pass
            if self._ff_proc is not None:
                try:
                    self._ff_proc.kill()
                except Exception:
                    pass
                self._ff_proc = None
            _log_info(f"[RTSP] ç»“æŸ: {self.rtsp_url}")
            self.connected = False

    def stop(self):
        self._stop.set()


class RTSPConsumerManager:
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self._threads = {}
        self._lock = threading.Lock()

    def start(self, stream_name: str, rtsp_url: str) -> bool:
        key = stream_name
        with self._lock:
            th = self._threads.get(key)
            if th and th.is_alive():
                # å·²æœ‰æ¶ˆè´¹è€…åœ¨è¿è¡Œ
                if getattr(th, 'rtsp_url', '') == rtsp_url:
                    return True
                # URL å˜åŒ–ï¼Œå…ˆåœæ­¢å†é‡å»º
                try:
                    th.stop()
                    th.join(timeout=1.0)
                except Exception:
                    pass
                self._threads.pop(key, None)
            elif th:
                # çº¿ç¨‹ä¸åœ¨è¿è¡Œï¼Œæ¸…ç†
                self._threads.pop(key, None)

            th_new = _ConsumerThread(stream_name, rtsp_url, self.model_manager)
            self._threads[key] = th_new
            th_new.start()
            return True

    def stop(self, stream_name: str) -> bool:
        key = stream_name
        with self._lock:
            th = self._threads.get(key)
            if th:
                try:
                    th.stop()
                    th.join(timeout=1.0)
                except Exception:
                    pass
                self._threads.pop(key, None)
                return True
            return False

    def status(self):
        now = time.time()
        info = {}
        for k, th in self._threads.items():
            info[k] = {
                'connected': getattr(th, 'connected', False),
                'last_frame_age_sec': (now - getattr(th, 'last_frame_ts', 0.0)) if getattr(th, 'last_frame_ts', 0.0) else None,
                'rtsp_url': getattr(th, 'rtsp_url', ''),
                'audio_started': getattr(th, '_audio_thread', None) is not None and getattr(th._audio_thread, 'is_alive', lambda: False)(),
                'audio_bytes': getattr(th, '_audio_bytes_read', 0),
                'audio_chunks': getattr(th, '_audio_chunks', 0),
                'audio_last_age_sec': (now - getattr(th, '_audio_last_ts', 0.0)) if getattr(th, '_audio_last_ts', 0.0) else None,
            }
        return info
